{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebPage Classifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJDsO1ghPXF4KW1JgrX0Ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivyul/WebPage-Classifier/blob/main/WebPage_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7NS06Phj-Tu",
        "outputId": "03d99d1c-4774-40ee-9ed2-38413b46262c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "Add all features\n",
        "Gradient Booting - part 4\n",
        "Embedded Matrix\n",
        "SQ learner\n",
        "SVM\n",
        "NLP - LSTM\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAdd all features\\nGradient Booting - part 4\\nEmbedded Matrix\\nSQ learner\\nSVM\\nNLP - LSTM\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZxjaqrSkaLk"
      },
      "source": [
        "Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts_pX2UxkTGP",
        "outputId": "8d520c41-a132-4952-9ad5-b33507a34728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn import svm\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from time import sleep\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2CPCkepke0g"
      },
      "source": [
        "TEST_SIZE = 10\n",
        "TRAIN_SIZE = 8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQisxscQkgpv"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_J9wJSrkf6z"
      },
      "source": [
        "# Functions\n",
        "def Convert(string):\n",
        "    li = list(string)\n",
        "    return li\n",
        "\n",
        "def extract(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    target_category = data.drop('url', axis='columns')\n",
        "    target_category = target_category.drop('html_file_name', axis='columns')\n",
        "    input_html = data.drop('url', axis='columns')\n",
        "    input_html = input_html.drop('category', axis='columns')\n",
        "    input_url = data.drop('category', axis='columns')\n",
        "    input_url = input_url.drop('html_file_name', axis='columns')\n",
        "    return target_category, input_html, input_url\n",
        "\n",
        "# Word Counts with CountVectorizer (bagOfWords)\n",
        "def tokenize(text):\n",
        "    vectorizer = TfidfVectorizer(text, stop_words='english')  # CountVectorizer() #CountVectorizer(document,stop_words='english') #TfidfVectorizer()\n",
        "    vectorizer.fit(text)\n",
        "    # print(vectorizer.vocabulary_)\n",
        "    vector = vectorizer.transform(text)\n",
        "    return vector\n",
        "\n",
        "# Get number of links in the page\n",
        "def NumOfLink(soup):\n",
        "    NumOfLink = 0\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        # print(\"Found the URL:\", a['href'])\n",
        "        NumOfLink += 1;\n",
        "    return NumOfLink\n",
        "\n",
        "\n",
        "def NumOfDirector(soup):\n",
        "    NumOfDirector = 0\n",
        "    for a in soup.find_all('Technical'):\n",
        "        print(\"director:\", a)\n",
        "        NumOfDirector += 1;\n",
        "    return NumOfDirector\n",
        "\n",
        "def GetHTMLTokenize(input_html):\n",
        "    html_list = []\n",
        "    for len in range(TEST_SIZE):  # input_html.__len__()):\n",
        "        html = input_html._get_value(len, 'html_file_name')\n",
        "        html = html.replace(\"_\", \" \")\n",
        "        html = html.replace(\".\", \" \")\n",
        "        html = html.replace(\"  \", \" \")\n",
        "        html_list = html_list + [html]\n",
        "    html_list = Convert(html_list)\n",
        "    return tokenize(html_list)\n",
        "\n",
        "def get_url(url):\n",
        "    print(url)\n",
        "    count = 0\n",
        "    page = ''\n",
        "    while page == '':\n",
        "        try:\n",
        "            page = requests.get(url)\n",
        "            break\n",
        "        except:\n",
        "            print(\"Connection refused by the server..\")\n",
        "            print(\"Let me sleep for 5 seconds\")\n",
        "            print(\"ZZzzzz...\")\n",
        "            time.sleep(5)\n",
        "            print(\"Was a nice sleep, now let me continue...\")\n",
        "            page = \"  \"\n",
        "            continue\n",
        "    return page\n",
        "\n",
        "\n",
        "def GetWebPageTextTokenizeAllText(input_url, remove_columns):\n",
        "    webpage_text = []\n",
        "    count = 0\n",
        "    for len in range(TEST_SIZE):  # input_url.__len__()):\n",
        "        print(count, ':')\n",
        "        html = get_url(input_url._get_value(830 + len, 'url'))\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')  # lxml is just the parser for reading the html\n",
        "        links = NumOfLink(soup)\n",
        "        wp = soup.get_text()\n",
        "        webpage_text = webpage_text + [wp]\n",
        "        count = count + 1\n",
        "        vector = tokenize(webpage_text)\n",
        "    df = pd.DataFrame(vector.todense())\n",
        "    print(\"Before remove features: \", df.shape)\n",
        "    if remove_columns:\n",
        "        sum_row = df.sum(axis=0)\n",
        "        print(type(sum_row))\n",
        "        cols = []\n",
        "        for idx in range(sum_row.size):\n",
        "            if (sum_row[idx] <= 2):\n",
        "                cols = cols + [idx]\n",
        "        df.drop(df.columns[cols], axis=1, inplace=True)\n",
        "    print(\"After remove features: \", df.shape)\n",
        "    return df\n",
        "\n",
        "\n",
        "def GetWebPageTextTokenize(input_url):\n",
        "    features = []\n",
        "    count = 0\n",
        "    for len in range(TEST_SIZE):  # input_url.__len__()):\n",
        "        print(count, ':')\n",
        "        html = get_url(input_url._get_value(830 + len, 'url'))\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')  # lxml is just the parser for reading the html\n",
        "        links = NumOfLink(soup)\n",
        "        text = soup.get_text()\n",
        "        numD = text.upper().count('director'.upper())\n",
        "        numM = text.upper().count('management'.upper())\n",
        "        numB = text.upper().count('board'.upper())\n",
        "        numT = text.upper().count('team'.upper())\n",
        "        numS = text.upper().count('staff'.upper())\n",
        "        numtrustees = text.upper().count('trustees'.upper())\n",
        "        numTreasurer = text.upper().count('Treasurer'.upper())\n",
        "        numgovernors = text.upper().count('governors'.upper())\n",
        "        numadvisors = text.upper().count('advisors'.upper())\n",
        "        numvisitors = text.upper().count('visitors'.upper())\n",
        "        numexecutive = text.upper().count('executive'.upper())\n",
        "        numCEO = text.upper().count('CEO'.upper())\n",
        "        numChairmen = text.upper().count('Chairmen'.upper())\n",
        "        f = [numD, numM, numB, numT, numS, numtrustees, numTreasurer, numgovernors, numadvisors, numvisitors,\n",
        "             numexecutive, numCEO, numChairmen, links]\n",
        "        features = features + [f]\n",
        "        count = count + 1\n",
        "        res = pd.DataFrame(data=features)\n",
        "    return res\n",
        "\n",
        "# determine the supported device\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') # don't have GPU\n",
        "    return device\n",
        "\n",
        "# convert a df to tensor to be used in pytorch\n",
        "def df_to_tensor(df):\n",
        "    device = get_device()\n",
        "    return torch.from_numpy(df.values).float().to(device)\n",
        "\n",
        "\n",
        "def pruning(model, X_train, Y_train):\n",
        "    # Post pruning decision trees with cost complexity pruning\n",
        "    path = model.cost_complexity_pruning_path(X_train, Y_train)\n",
        "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "    clfs = []\n",
        "    for ccp_alpha in ccp_alphas:\n",
        "        clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        clfs.append(clf)\n",
        "    print(\n",
        "        \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
        "    # Accuracy vs alpha for training and testing sets\n",
        "    train_scores = [clf.score(X_train, Y_train) for clf in clfs]\n",
        "    test_scores = [clf.score(X_test, Y_test) for clf in clfs]\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(\"alpha\")\n",
        "    ax.set_ylabel(\"accuracy\")\n",
        "    ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "    ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
        "            drawstyle=\"steps-post\")\n",
        "    ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
        "            drawstyle=\"steps-post\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    return ccp_alpha"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fIFCZ-k-zv"
      },
      "source": [
        "DD Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-nsnqY7lCnZ"
      },
      "source": [
        "class NetName(nn.Module):\n",
        "\n",
        "    def __init__(self, D_in, D_out):\n",
        "        H1 = 10\n",
        "        H2 = 10\n",
        "        super(NetName, self).__init__()\n",
        "        # Number of input features is D_in.\n",
        "        self.layer_1 = nn.Linear(D_in, 64)\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDlqgFlljGXI",
        "outputId": "1838b4d3-4301-4f21-ec00-5f215990b0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Functions\n",
        "def Convert(string):\n",
        "    li = list(string)\n",
        "    return li\n",
        "\n",
        "\n",
        "def extract(file_name):\n",
        "    data = pd.read_csv(file_name)\n",
        "    target_category = data.drop('url', axis='columns')\n",
        "    target_category = target_category.drop('html_file_name', axis='columns')\n",
        "    input_html = data.drop('url', axis='columns')\n",
        "    input_html = input_html.drop('category', axis='columns')\n",
        "    input_url = data.drop('category', axis='columns')\n",
        "    input_url = input_url.drop('html_file_name', axis='columns')\n",
        "    return target_category, input_html, input_url\n",
        "\n",
        "\n",
        "# Word Counts with CountVectorizer (bagOfWords)\n",
        "def tokenize(text):\n",
        "    vectorizer = TfidfVectorizer(text, stop_words='english')  # CountVectorizer() #CountVectorizer(document,stop_words='english') #TfidfVectorizer()\n",
        "    vectorizer.fit(text)\n",
        "    # print(vectorizer.vocabulary_)\n",
        "    vector = vectorizer.transform(text)\n",
        "    return vector\n",
        "\n",
        "\n",
        "# Get number of links in the page\n",
        "def NumOfLink(soup):\n",
        "    NumOfLink = 0\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        # print(\"Found the URL:\", a['href'])\n",
        "        NumOfLink += 1;\n",
        "    return NumOfLink\n",
        "\n",
        "\n",
        "def NumOfDirector(soup):\n",
        "    NumOfDirector = 0\n",
        "    for a in soup.find_all('Technical'):\n",
        "        print(\"director:\", a)\n",
        "        NumOfDirector += 1;\n",
        "    return NumOfDirector\n",
        "\n",
        "\n",
        "def GetHTMLTokenize(input_html):\n",
        "    html_list = []\n",
        "    for len in range(TEST_SIZE):  # input_html.__len__()):\n",
        "        html = input_html._get_value(len, 'html_file_name')\n",
        "        html = html.replace(\"_\", \" \")\n",
        "        html = html.replace(\".\", \" \")\n",
        "        html = html.replace(\"  \", \" \")\n",
        "        html_list = html_list + [html]\n",
        "    html_list = Convert(html_list)\n",
        "    return tokenize(html_list)\n",
        "\n",
        "\n",
        "def get_url(url):\n",
        "    print(url)\n",
        "    count = 0\n",
        "    page = ''\n",
        "    while page == '':\n",
        "        try:\n",
        "            page = requests.get(url)\n",
        "            break\n",
        "        except:\n",
        "            print(\"Connection refused by the server..\")\n",
        "            print(\"Let me sleep for 5 seconds\")\n",
        "            print(\"ZZzzzz...\")\n",
        "            time.sleep(5)\n",
        "            print(\"Was a nice sleep, now let me continue...\")\n",
        "            page = \"  \"\n",
        "            continue\n",
        "    return page\n",
        "\n",
        "\n",
        "def GetWebPageTextTokenizeAllText(input_url, remove_columns):\n",
        "    webpage_text = []\n",
        "    count = 0\n",
        "    for len in range(TEST_SIZE):  # input_url.__len__()):\n",
        "        print(count, ':')\n",
        "        html = get_url(input_url._get_value(830 + len, 'url'))\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')  # lxml is just the parser for reading the html\n",
        "        links = NumOfLink(soup)\n",
        "        wp = soup.get_text()\n",
        "        webpage_text = webpage_text + [wp]\n",
        "        count = count + 1\n",
        "        vector = tokenize(webpage_text)\n",
        "    df = pd.DataFrame(vector.todense())\n",
        "    print(\"Before remove features: \", df.shape)\n",
        "    if remove_columns:\n",
        "        sum_row = df.sum(axis=0)\n",
        "        print(type(sum_row))\n",
        "        cols = []\n",
        "        for idx in range(sum_row.size):\n",
        "            if (sum_row[idx] <= 2):\n",
        "                cols = cols + [idx]\n",
        "        df.drop(df.columns[cols], axis=1, inplace=True)\n",
        "    print(\"After remove features: \", df.shape)\n",
        "    return df\n",
        "\n",
        "\n",
        "def GetWebPageTextTokenize(input_url):\n",
        "    features = []\n",
        "    count = 0\n",
        "    for len in range(TEST_SIZE):  # input_url.__len__()):\n",
        "        print(count, ':')\n",
        "        html = get_url(input_url._get_value(830 + len, 'url'))\n",
        "        soup = BeautifulSoup(html.text, 'html.parser')  # lxml is just the parser for reading the html\n",
        "        links = NumOfLink(soup)\n",
        "        text = soup.get_text()\n",
        "        numD = text.upper().count('director'.upper())\n",
        "        numM = text.upper().count('management'.upper())\n",
        "        numB = text.upper().count('board'.upper())\n",
        "        numT = text.upper().count('team'.upper())\n",
        "        numS = text.upper().count('staff'.upper())\n",
        "        numtrustees = text.upper().count('trustees'.upper())\n",
        "        numTreasurer = text.upper().count('Treasurer'.upper())\n",
        "        numgovernors = text.upper().count('governors'.upper())\n",
        "        numadvisors = text.upper().count('advisors'.upper())\n",
        "        numvisitors = text.upper().count('visitors'.upper())\n",
        "        numexecutive = text.upper().count('executive'.upper())\n",
        "        numCEO = text.upper().count('CEO'.upper())\n",
        "        numChairmen = text.upper().count('Chairmen'.upper())\n",
        "        f = [numD, numM, numB, numT, numS, numtrustees, numTreasurer, numgovernors, numadvisors, numvisitors,\n",
        "             numexecutive, numCEO, numChairmen, links]\n",
        "        features = features + [f]\n",
        "        count = count + 1\n",
        "        res = pd.DataFrame(data=features)\n",
        "    return res\n",
        "\n",
        "# determine the supported device\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') # don't have GPU\n",
        "    return device\n",
        "\n",
        "# convert a df to tensor to be used in pytorch\n",
        "def df_to_tensor(df):\n",
        "    device = get_device()\n",
        "    return torch.from_numpy(df.values).float().to(device)\n",
        "\n",
        "\n",
        "def pruning(model, X_train, Y_train):\n",
        "    # Post pruning decision trees with cost complexity pruning\n",
        "    path = model.cost_complexity_pruning_path(X_train, Y_train)\n",
        "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "    clfs = []\n",
        "    for ccp_alpha in ccp_alphas:\n",
        "        clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "        clf.fit(X_train, Y_train)\n",
        "        clfs.append(clf)\n",
        "    print(\n",
        "        \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
        "    # Accuracy vs alpha for training and testing sets\n",
        "    train_scores = [clf.score(X_train, Y_train) for clf in clfs]\n",
        "    test_scores = [clf.score(X_test, Y_test) for clf in clfs]\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(\"alpha\")\n",
        "    ax.set_ylabel(\"accuracy\")\n",
        "    ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "    ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
        "            drawstyle=\"steps-post\")\n",
        "    ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
        "            drawstyle=\"steps-post\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "    return ccp_alpha\n",
        "\n",
        "\n",
        "\n",
        "class NetName(nn.Module):\n",
        "\n",
        "    def __init__(self, D_in, D_out):\n",
        "        H1 = 10\n",
        "        H2 = 10\n",
        "        super(NetName, self).__init__()\n",
        "        # Number of input features is D_in.\n",
        "        self.layer_1 = nn.Linear(D_in, 64)\n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        return x\n",
        "\n",
        "# Read .csv file\n",
        "ret = extract(\"dataset.csv\")\n",
        "target_category = ret[0]\n",
        "input_html = ret[1]\n",
        "input_url = ret[2]\n",
        "\n",
        "# Get vectors for url, html file name (inputs) and category (target)\n",
        "vectorHTML = GetHTMLTokenize(input_html)\n",
        "df_HTML = pd.DataFrame(vectorHTML.todense())\n",
        "df_WP_1 = GetWebPageTextTokenizeAllText(input_url, 1)\n",
        "df_WP_2 = GetWebPageTextTokenize(input_url)\n",
        "#\n",
        "inputs = pd.concat([df_WP_1, df_WP_2, df_HTML], axis=1)\n",
        "\n",
        "# Export DataFrame\n",
        "inputs.to_csv(\"SpecificFeatures_.csv\")\n",
        "# Import DataFrame\n",
        "# inputs1 = pd.read_csv(\"AllFeatures_700_0.85.csv\")\n",
        "# inputs = inputs1.drop(inputs1.columns[[0]], axis=1)\n",
        "\n",
        "target_dictionary = ['BOD', 'MTEAM_BOD', 'DIRECTORY URL', 'MTEAM', 'OTHER']\n",
        "vectorTarget = []\n",
        "for x in range(len(target_category)):\n",
        "    if (target_category.at[x, 'category'] == 'BOD') or (target_category.at[x, 'category'] == 'MTEAM_BOD'):\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    vectorTarget = vectorTarget + [val]\n",
        "# le_target = LabelEncoder();\n",
        "# vectorTarget = le_target.fit_transform(target_category)\n",
        "target_category_t = vectorTarget[:TRAIN_SIZE]\n",
        "inputs_t = inputs.head(TRAIN_SIZE)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(inputs_t, target_category_t, test_size=.3, random_state=1)\n",
        "\n",
        "net = NetName(len(X_train.columns),1)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.01) # keep momentum per batch?\n",
        "loss_fn = nn.L1Loss()\n",
        "n_epochs = 500\n",
        "for t in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    X_train_tensor = df_to_tensor(X_train)\n",
        "    idx = 0\n",
        "    # for param in net.parameters():\n",
        "    #     idx += 1\n",
        "    #     print(type(param.data), param.size())\n",
        "    #     print('param: ', idx, '\\n', param.data)\n",
        "    Y_train_tensor_het = net(X_train_tensor)\n",
        "    Y_train_tensor = torch.tensor(Y_train)\n",
        "    loss = loss_fn(Y_train_tensor, Y_train_tensor_het)\n",
        "    # print('loss: ', loss)\n",
        "    if loss > 0.05:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        print(\"Stop Training after:\", t)\n",
        "        print('Training loss is: ', loss)\n",
        "        break\n",
        "\n",
        "GB_model = GradientBoostingClassifier(n_estimators=150)\n",
        "GB_model.fit(X_train, Y_train)\n",
        "y_pred_train = GB_model.predict(X_train)\n",
        "y_pred = GB_model.predict(X_test)\n",
        "print('GB_model score: ',GB_model.score(X_train, Y_train))\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score Gradient Boosting: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error Gradient Boosting: ', err)\n",
        "\n",
        "\n",
        "GB_model = GradientBoostingClassifier(n_estimators=180)\n",
        "GB_model.fit(X_train, Y_train)\n",
        "y_pred_train = GB_model.predict(X_train)\n",
        "y_pred = GB_model.predict(X_test)\n",
        "print('GB_model score: ',GB_model.score(X_train, Y_train))\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score Gradient Boosting: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error Gradient Boosting: ', err)\n",
        "\n",
        "GB_model = GradientBoostingClassifier(n_estimators=200)\n",
        "GB_model.fit(X_train, Y_train)\n",
        "y_pred_train = GB_model.predict(X_train)\n",
        "y_pred = GB_model.predict(X_test)\n",
        "print('GB_model score: ',GB_model.score(X_train, Y_train))\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score Gradient Boosting: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error Gradient Boosting: ', err)\n",
        "\n",
        "\n",
        "DT_model = tree.DecisionTreeClassifier(random_state=1)\n",
        "DT_model.fit(X_train, Y_train)\n",
        "ccp_alphas = pruning(DT_model, X_train, Y_train)\n",
        "DT_model = tree.DecisionTreeClassifier(random_state=1, ccp_alpha=0.0039)\n",
        "DT_model.fit(X_train, Y_train)\n",
        "y_pred_train = DT_model.predict(X_train)\n",
        "print('DT_model score: ',DT_model.score(X_train, Y_train))\n",
        "y_pred = DT_model.predict(X_test)\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score DecisionTree: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error DecisionTree: ', err)\n",
        "\n",
        "RF_model = RandomForestClassifier(random_state=1)\n",
        "RF_model.fit(X_train, Y_train)\n",
        "y_pred_train = RF_model.predict(X_train)\n",
        "y_pred = RF_model.predict(X_test)\n",
        "print('RF_model score: ',RF_model.score(X_train, Y_train))\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score Random Forest: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error Random Forest: ', err)\n",
        "\n",
        "NB_model = MultinomialNB()\n",
        "NB_model.fit(X_train, Y_train)\n",
        "y_pred_train = NB_model.predict(X_train)\n",
        "print('NB_model score: ',NB_model.score(X_train, Y_train))\n",
        "y_pred = NB_model.predict(X_test)\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score MultinomialNB: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error MultinomialNB: ', err)\n",
        "\n",
        "LR_model = LogisticRegression()\n",
        "LR_model.fit(X_train, Y_train)\n",
        "y_pred_train = LR_model.predict(X_train)\n",
        "print('LR_model score: ',LR_model.score(X_train, Y_train))\n",
        "y_pred = LR_model.predict(X_test)\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score LR_model: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error LR_model: ', err)\n",
        "\n",
        "SVM_model = svm.LinearSVC()\n",
        "SVM_model.fit(X_train, Y_train)\n",
        "y_pred_train = SVM_model.predict(X_train)\n",
        "print('SVM_model score: ',SVM_model.score(X_train, Y_train))\n",
        "y_pred = SVM_model.predict(X_test)\n",
        "err = accuracy_score(Y_test, y_pred)\n",
        "print('accuracy_score SVM_model: ', err)\n",
        "err = mean_absolute_error(Y_test, y_pred)\n",
        "print('mean_absolute_error SVM_model: ', err)\n",
        "X_train_tensor = df_to_tensor(X_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0ebedc925b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Read .csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-286f47124dde>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtarget_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_category\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'html_file_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsZIDI0_jTRI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TluclvzWjTjl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}